[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Hersh and I‚Äôm a data scientist based in Boston, MA and I love to use data to make an impact on policy, processes, and government.\nAll opinions expressed in this blog represent my own.\nReach out to me here! üëáüèæ:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "I Didn‚Äôt Leave Public Service, Public Service Left Me: Why Inefficient Government Hiring Processes Drove Me Away\n\n\n\n\n\n\nEmployment\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I Learned Building CityUpdate: An AI-Powered Data Product for Public Sector Transparency\n\n\n\n\n\n\nGovernment\n\n\nAI\n\n\nLLM\n\n\n\nWhat I learned building CityUpdate, an AI powered data product for the public sector\n\n\n\n\n\nFeb 12, 2025\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Smarter 311 Systems with Vision Models\n\n\n\n\n\n\nGovernment\n\n\nAI\n\n\nLMM\n\n\n\nEnhance city service requests with AI\n\n\n\n\n\nDec 14, 2024\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nLocal Vision-Language Model Experimentation for Public Sector Use Cases\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\nAI\n\n\n\n\n\n\n\n\n\nNov 27, 2024\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Most State and Local Generative AI Efforts Won‚Äôt Be as Impactful as Promised\n\n\n\n\n\n\nGovernment\n\n\nAI\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nObservable‚Äôs New Framework: A Better Way for Cities to Tell Data Stories\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nHow Cities Can Create Better Data Interfaces: Lessons Learned from Redesigning RentSmart Boston\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nHow government analytics teams can embrace #PublicMoneyPublicCode\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nApr 2, 2023\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nCity Government Open Source Efforts, Ranked by Their Github Stats\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nDec 22, 2021\n\n\nHersh Gupta\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/city-government-open-source/index.html",
    "href": "posts/city-government-open-source/index.html",
    "title": "City Government Open Source Efforts, Ranked by Their Github Stats",
    "section": "",
    "text": "In 2016, then US Chief Information Officer Tony Scott announced the launch code.gov, stating\n\n‚ÄúBy harnessing 21st Century technology and innovation, we‚Äôre improving the Federal Government‚Äôs ability to provide better citizen-centered services and are making the Federal Government smarter, savvier, and more effective for the American people‚Ä¶We also envision it becoming a useful resource for State and local governments and developers looking to tap into the Government‚Äôs code to build similar services, foster new connections with their users, and help us continue to realize the President‚Äôs vision for a 21st Century digital government.‚Äù\n\nSince then, many cities have made the push to open source, with more and more opting to publish their code in public-facing repositories.\n\nOpen-Source Cities, Ranked\nI rank US municipal governments‚Äô in how well they have adapted to the new landscape of open source government code, with the following caveats:\n\nLimited to the 100 largest U.S. cities by population\nOfficial government Github profiles, no private third-party or volunteer organizations\nPublic repositories only\n\n\n\n\n\n\n\n\n\nHow this works: City open-source rankings are calculated by analyzing public metrics available via the Github Repository Metrics API endpoint. Scores are calculated by combining all repository metrics, including total number of forks, watchers, and commits along with the total number of public repositories on each city‚Äôs Github profile. Code for this analysis is available on Github here."
  },
  {
    "objectID": "posts/government-analytics-oss/index.html",
    "href": "posts/government-analytics-oss/index.html",
    "title": "How government analytics teams can embrace #PublicMoneyPublicCode",
    "section": "",
    "text": "The push for Public Money, Public Code started in Europe with the aim that government software developed for the public sector should be made publicly available via a Free and Open Source Software (FOSS) license. In fact, there are economic, moral, transparency, and participatory related reasons why governments should use and publish open source software. Recently, bipartisan legislation has been introduced in the US Senate that would, among other things, ‚Äúcodify open source software as public infrastructure.‚Äù\nMuch of the focus of Public Money, Public Code has been to allow government agencies to develop and contribute to open source software applications. Government analytics teams, on the other hand, do not consider themselves software developers. However, the same way that scientific research funded via public grants is made publicly available, government analytics teams can opt to use, promote, and orchestrate with open source tools. Thus, they can reap the same benefits as other organizations that have chosen to be a part of the open source community.\nA ‚Äúdata stack‚Äù is a set of interoperable tools used for analytic data work. The table below shows open source alternatives to proprietary analytics tools:\n\nOSS Alternatives\n\n\n\n\n\n\n\nPurpose\nProprietary Tools\nOpen Source Tools\n\n\n\n\nInfrastructure Orchestration\nAnsible\nDocker, Kubernetes\n\n\nResearch\nSTATA, SAS, SPSS\nR, Python\n\n\nBusiness Intelligence (Internal)\nTableau, PowerBI, Microstrategy\nApache Superset\n\n\nDashboards (External)\nTableau, ArcMap, PowerBI\nJavaScript\n\n\nGIS\nArcGIS Desktop\nPostGIS, Apache Sedona\n\n\nETL/Pipelines\nInformatica, Tableau Prep\nAirflow, Dagster, Meltano\n\n\nAnalytical Query Engine\nPL/SQL\nTrino\n\n\nReal-Time Analytics\n?\nApache Pinot\n\n\n\nUsing an open source data stack can help government analytics teams embrace #PublicMoneyPublicCode. Many proprietary/paid/‚ÄúEnterprise‚Äù tools will claim to be more stable and secure than open source tools. However, open source tools are not inherently insecure, as open source technology is becoming the bedrock of more and more Enterprise tools.\nThe cash cow of companies that develop closed-source software is the generalization that ‚Äúgovernment users are more risk-averse and will stick with closed source software.‚Äù This is best refuted by the fact that the US Department of Defense has published a Frequently Asked Questions page on open source software and states ‚ÄúSome OSS is very secure, while others are not; some proprietary software is very secure, while others are not. Each product must be examined on its own merits.‚Äù As the US Department of Defense has publicly embraced OSS, so too can government analytics teams at all levels of government."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "3 Tips for Better Data Analysis Using Strategies from Software Development\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#borrow-from-agile",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#borrow-from-agile",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "1. Borrow from Agile",
    "text": "1. Borrow from Agile\nAgile is a process framework that focuses on iterative development done in short bursts, called sprints\nPlanning and prioritization at the start of each sprint\nUse regular scrum meetings to conduct sprint planning, prioritization, and review\nClearly defining tasks with deliverables and timelines\nPresent hypotheses, approach(es), and aspects of data product(s)\nRetrospectives and demos at the end of each sprint\nPresent analyses/data products to stakeholders, review sprint"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#use-readmes",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#use-readmes",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "2. Use READMEs",
    "text": "2. Use READMEs\nA README is documentation that tells you how to start using and understanding a new piece of software\nA typical README is a text or markdown-formatted file included with the project\nLearn how to format a README here: https://www.makeareadme.com/\nREADMEs for data products include things like usage, reproducibility, sources, and metadata"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#integrate-user-experience-testing",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#integrate-user-experience-testing",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "3. Integrate User Experience Testing",
    "text": "3. Integrate User Experience Testing\nFor a widely-used data product, consider User Stories\nUser Stories are a prioritized list of tasks the user is trying to accomplish\nUser Stories help developers, analysts understand users‚Äô needs\nIterative, human centered design\nIs the analysis interpretable? Is the way it‚Äôs presented intuitive and simple?\nPrototype analysis with user groups, have them ask questions, document findings"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#tips-for-better-data-analysis",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#tips-for-better-data-analysis",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "Tips for Better Data Analysis",
    "text": "Tips for Better Data Analysis\n\nBorrow from Agile\n\nUse READMEs\n\nIntegrate User Experience Testing"
  },
  {
    "objectID": "posts/redesigning-rent-smart-boston/index.html",
    "href": "posts/redesigning-rent-smart-boston/index.html",
    "title": "How Cities Can Create Better Data Interfaces: Lessons Learned from Redesigning RentSmart Boston",
    "section": "",
    "text": "tl;dr: redesigning the Boston RentSmart dashboard helped me understand when and why cities should choose to build custom web interfaces for datasets.\nThe early 2010s featured a surge of ‚Äúopen data‚Äù ‚Äì from government datasets to public research. But while this wealth of data has potential value, a question remains around how to make it truly useful and accessible.\nOn one hand, data analysts rely on tools to efficiently process datasets, but these rarely enable building new interfaces. They use basic publishing tools to share findings. Web developers, on the other hand, focus on crafting engaging interfaces but often lack access to rich datasets. There‚Äôs a divide between these roles - one cares about data maintenance, the other about display. Bridging this gap between data and design experts is key to unlocking the promise of open data for the public good.\nTo personally bridge this gap in my own skillset, I learned enough web development to redesign the City of Boston‚Äôs RentSmart Dashboard. For some context, The City of Boston provides compiled data on housing violations, building violations, enforcement violations, housing complaints, sanitation requests, and/or civic maintenance requests. The City releases this data through¬†Analyze Boston¬†and they provide a¬†Tableau dashboard¬†to allow users to search the data.\nI redesigned this dashboard in a modern web framework, with similar functionality to the original one. Here‚Äôs what I learned:\n\nInvesting time in custom UI and UX is worthwhile if you care about user experience. Building an interface from scratch takes more effort upfront, but offers far more flexibility to craft an intuitive, user-friendly product. Off-the-shelf BI tools often lack the polish and refinements of a bespoke web app.\nWeb development scales better long-term than relying solely on point-and-click BI tools. Web apps can be optimized for performance, accessibility, SEO, and more through standard practices. BI tools don‚Äôt often receive this level of optimization. Additionally, thanks to reusable code patterns, each new web app moves faster than starting a dashboard from scratch. The reproducibility of code compounds over time.\nWeb frameworks promote collaboration and community growth. By using open source tools and publishing my code, I can get feedback, bug reports, and contributions from other developers. The open source community helps improve the quality and security of the product through peer review and knowledge sharing. Proprietary BI tools tend to have less community support.\n\nBy diving into web development to create a more engaging open data resource, I gained hands-on experience bridging the gap between my data analysis skills and ability to build great interfaces. Custom web development ultimately creates a better user experience and offers more scalability than inflexible, out-of-the-box BI tools alone."
  },
  {
    "objectID": "posts/city-data-stories-with-observable/index.html",
    "href": "posts/city-data-stories-with-observable/index.html",
    "title": "Observable‚Äôs New Framework: A Better Way for Cities to Tell Data Stories",
    "section": "",
    "text": "Testing Observable 2.0 with Boston's 311 Data\n\n\nCities today want to leverage their vast amounts of open data to tell compelling stories that inform, optimize operations, and engage citizens. However, it is too expensive for most cities to invest in entire web teams to create customized dashboards. The answer lies in open source software and embracing code. Open source options allow cities to save significantly on software licensing costs, while a code based approach facilitates reproducibility and, thus, economies of scale.\nOne open source framework that is gaining popularity for data applications is Observable. Observable combines the power of languages like Python, R, and SQL that data scientists rely on with JavaScript, the native language of the web. This allows cities to create highly dynamic and interactive data visualizations that live natively on the web, removing the need for proprietary business intelligence or visualization software.\nWith Observable 2.0, interfaces are built locally and available to deploy cites‚Äô own servers. This way, users don‚Äôt have to transmit sensitive data to third-party servers. Instead, they maintain full control over security and privacy while still producing powerful analytics.\n\n\n\nWhich URL is SEO optimized? Which would you expect to load more quickly?\n\n\nThe new Observable Framework is also extremely customizable, enabling cities to build precisely the dashboards and data stories they wish to tell, optimized specifically for their use cases. Compared to traditional government software options like ArcGIS and Tableau, Observable provides better performance at a fraction of the cost. With reusable data analysis and visualization components, cities can quickly build multiple data applications on top of the same foundational codebase. This modular approach paired with the latest web development techniques makes Observable a cost-effective way for cities to tell a story with their data.\nBy embracing open source software like Observable, cities access ready-made tools for success while benefiting from standardized best practices. Observable allows cities to tell rich, interactive analytic narratives with their public sector data in a secure, performant manner that simply isn‚Äôt possible with other approaches. Any city looking to innovate with data visualization and storytelling would do well to explore this new framework."
  },
  {
    "objectID": "posts/state-local-gov-unready-for-genAI/index.html",
    "href": "posts/state-local-gov-unready-for-genAI/index.html",
    "title": "Why Most State and Local Generative AI Efforts Won‚Äôt Be as Impactful as Promised",
    "section": "",
    "text": "I'm a data scientist working on AI in city government. I think hype around generative AI in the public sector is causing more issues than it promises to solve.\nDevelopments in generative artificial intelligence seem promising in improving government operations, like streamlining routine bureaucratic tasks or summarizing documents for information-seekers. However, the reality is that most state and local governments lack the expertise, safeguards, and tech leadership to implement products or processes that integrate generative AI.\nDespite the unpreparedness, cities are jumping on the generative AI bandwagon. It's spurred by technology managers falling for flashy claims of cutting-edge public sector innovations from B2G vendors.\nA common trope in government is that government technology leaders don't understand the \"business\" of government, i.e., they are too far removed from the day-to-day of government services. This leads to AI chatbots being seen by these leaders as a sort of panacea to most government challenges, without understanding the nuances of a given problem.\nIt's complicated further by the fact that few, if any, government technology managers deeply understand the underlying tech of generative AI. The implication here is that, while the typical gov tech leader is aware that genAI has drawbacks, leaders lack the knowledge to adequately measure or mitigate them. New York City's recent genAI scandal serves as a prime example.\nBecause of this, the public should be more skeptical of the people in government who push generative AI technologies without understanding them or the problems they're actually able to solve.\nRather than chasing the latest AI innovations, govtech managers would be better served prioritizing foundational data management practices - establishing centralized governance, systematizing data collection/operations, and thoughtfully designing core IT infrastructure.\nHowever, hype around AI and other advanced technologies creates pressure on local leaders to implement them before doing the foundational work. The allure of AI can push limited resources toward shiny new projects rather than the admittedly boring work of improving core systems and infrastructure.\nIf asked whether they should hire another database administrator or an procure some AI software, too many gov leaders would be mistakenly convinced the smarter investment would be expanding AI capacity.\nIn their rush to keep up with innovation for innovation's sake, many states and cities will try to leapfrog over essential steps and end up with serious consequences for the public and government operations. Potentially useful but unreliable AI systems, when deployed without appropriate forethought, will only amplify existing operational inefficiencies rather than resolving them.\nReal innovation demands some level of care. Resisting the temptation to compulsively chase every shiny new AI object will yield more substantial, long-term dividends for residents."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html",
    "href": "posts/local-lmms-in-government/index.html",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "",
    "text": "Computer vision has evolved many times, from basic handwriting pattern recognition to object detection. Now, with the emergence of Large Multimodal Model (LMM), it‚Äôs experiencing another dramatic transformation. LMMs can now process images and generate descriptions, insights, analysis and even new prompts for other models. Large (and even mid-sized) tech companies have the resources to deploy these models in the cloud and support them with teams dedicated to their upkeep. Government, however, has long lacked this luxury ‚Äì there are very few ML/AI teams in government, and where they exist are in already well-funded agencies and organizations. Therefore, there‚Äôs a compelling case for experimenting with these models while taking the lowest-cost, lowest-risk approach."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#vision-models-in-government",
    "href": "posts/local-lmms-in-government/index.html#vision-models-in-government",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Vision Models in Government",
    "text": "Vision Models in Government\nGovernments sit on goldmines of visual data: from infrastructure inspection photos to document processing queues, from aerial imagery to camera feeds. These are another form of administrative data, the utility of which often gets overlooked. The potential for automation and efficiency gains is plenty. For example, a vision model could rapidly assess road damage from maintenance photos, extract data from handwritten forms, or analyze aerial imagery for urban planning.\nWhile the public sector would benefit from any of these applications, it often lacks the technical resources to implement them effectively. Government agencies typically employ few experts or engineers compared to the private sector, and the collaboration between technical teams and services designers is not as tightly coupled in the public sector as it is in the private sector."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#the-case-for-local-experimentation",
    "href": "posts/local-lmms-in-government/index.html#the-case-for-local-experimentation",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "The Case for Local Experimentation",
    "text": "The Case for Local Experimentation\nCloud platform LMM offerings, while powerful, come with significant costs that can strain public sector budgets. Each Rekognition or AI Vision API call adds up, making experimentation and proof-of-concept development prohibitively expensive. This is compounded by the fact that government agencies do not have large, experienced engineering teams, so they end up spend more time spinning their wheels while accruing these costs.\nThis is where local deployment of models becomes a practical alternative. Local vision models, specifically, offer several advantages:\n\nCost-effective experimentation without charges for each inference/query\nComplete control over data privacy and security\nFreedom to modify and fine-tune models for specific use cases\n\nThere are a few downsides: you can‚Äôt use proprietary models (GPT/Claude are off the table), you need to have the right hardware (but honestly, if you‚Äôre already buying a $2K computer for a new hire, might as well throw in a GPU), and it‚Äôs not as easy as point-and-click interfaces."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#lmm-public-sector-applications",
    "href": "posts/local-lmms-in-government/index.html#lmm-public-sector-applications",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "LMM Public Sector Applications",
    "text": "LMM Public Sector Applications\nThe applications for vision models in government services are varied:\n\nGenerating Service Tickets: Analyzing images of incidents or areas needing service and preparing reports to 311\nInfrastructure Maintenance: Analyzing photos of parks, roads, and buildings to identify maintenance needs\nEnvironmental Monitoring: Processing satellite and drone imagery to track environmental changes\nAccessibility Services: Providing image descriptions for visually impaired citizens accessing government websites"
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#example-open-source-vision-intelligence-with-llava",
    "href": "posts/local-lmms-in-government/index.html#example-open-source-vision-intelligence-with-llava",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Example: Open Source Vision Intelligence with LLaVA",
    "text": "Example: Open Source Vision Intelligence with LLaVA\nThere are hundreds of open-source options for image-to-text models. However, LLaVA is one that stands out for its capabilities and accessibility. Built on Meta‚Äôs LLaMA language model, LLaVA uses specialized components to translate images into a format the underlying language model can process, allowing it to have natural conversations about visual content.\nLLaVA-NeXT represents a significant step forward in making local vision models more accessible. It uses the latest family of LLaMA models, ‚Äúwith improved reasoning, OCR, and world knowledge.‚Äù"
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#proof-of-concept-experiments",
    "href": "posts/local-lmms-in-government/index.html#proof-of-concept-experiments",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Proof-of-Concept Experiments",
    "text": "Proof-of-Concept Experiments\nI tested LLaVA-NeXT on two common municipal workflows: 311 service requests and playground safety inspections. Both experiments used specialized system prompts to structure the model‚Äôs analysis.\n\n311 Service Request Processing\nThe model converted photos into structured tickets by extracting location data, classifying issues, and generating detailed descriptions.\n\n\n\nTesting LLaMA-NeXT with picture of traffic in lmstudio.ai\n\n\nIn testing with traffic obstruction photos, it correctly identified violations and provided comprehensive situation assessments matching human operator quality.\n\n\nPlayground Safety Inspections\nFor park maintenance, the model performed systematic equipment evaluations. When analyzing playground components, it reliably assessed material condition, structural integrity, and potential safety hazards.\n\n\n\nTesting LLaMA-NeXT with picture of a playground in lmstudio.ai\n\n\nThe output followed given inspection protocols, providing maintenance teams with actionable documentation.\n\n\nKey Findings\nLocal deployment of LLaVA-NeXT proved effective, with several advantages:\n\nFast processing time on consumer hardware (my laptop‚Äôs GPU)\nConsistent output formatting suitable for existing workflows\nSuccessful domain adaptation through prompt engineering\nNo additional training required for municipal use cases\n\nThese experiments demonstrate that local LMMs can effectively augment government workflows while maintaining control over data and infrastructure."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#conclusion",
    "href": "posts/local-lmms-in-government/index.html#conclusion",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Conclusion",
    "text": "Conclusion\nThe democratization of vision model technology through local deployment options opens exciting possibilities for public sector innovation. While cloud-based solutions will continue to play a crucial role, the ability to experiment locally removes significant barriers to entry for government agencies looking to improve their services.\nBy starting with focused proof-of-concept projects using tools like LLaVA-NeXT, public sector organizations can build the expertise and confidence needed to implement these transformative technologies at scale. The key is to begin with manageable experiments that demonstrate clear value while building internal capacity for larger implementations.\nCover image (c) karyative, SmashIcons"
  },
  {
    "objectID": "posts/smarter-311-with-vision-models/index.html",
    "href": "posts/smarter-311-with-vision-models/index.html",
    "title": "Building Smarter 311 Systems with Vision Models",
    "section": "",
    "text": "Every day, residents of any given city might submit hundreds of 311 requests for everything from broken sidewalks to illegal parking. These systems are important in responding to a city‚Äôs service requests.\nThe process seems simple: a resident reports an issue, and the appropriate department responds. But behind the scenes, 311 employees have to constantly recategorize misclassified requests and merge duplicate reports about the same problem.\nA pothole might be reported as ‚Äústreet damage‚Äù by one resident and ‚Äúroad hazard‚Äù by another, requiring manual intervention to connect these related cases.\nArtificial intelligence‚Äîspecifically large multimodal models (LMMs)‚Äîcould transform 311 operations.\nWhy use AI?\nIt‚Äôs not just about technological efficiency‚Äîit‚Äôs about creating tangible improvements in city services. When a broken traffic signal is reported, accurate categorization means faster response times. When multiple residents report the same fallen tree, automatic merging ensures resources aren‚Äôt split across duplicate tickets.\nImplementing LMMs in 311 systems would be like a phone‚Äôs autocorrect, but for service requests. As users are submitting requests, it would:\n\nSuggest the correct department classification\nVerify the issues via the images that are submitted\nFlag emergency situations for immediate attention\nIdentify and link related requests in the system\n\nIdeally, users would be able to verify the suggestions before submitting their requests, so there‚Äôs less danger of AI slop ending up in 311 systems.\nHow does this work?\nA big problem with integrating AI systems is getting consistent outputs. That‚Äôs why the folks at StanfordNLP have built DSPy ‚Äì a framework for writing programs for language model workflows.\nUsing DSPy, we can create a reliable pipeline for processing 311 requests that takes the request description text and images, and generated an updated request:\n\noriginal_request: list -&gt; updated_request: list\n\nI‚Äôve created an example implementation that demonstrates these concepts, available in the Smart 311 System repository. The repository includes both a simple standalone example and a more complete implementation showing how to structure a production-ready system.\nHere‚Äôs how it works in practice. First, we configure the LMM ‚Äì here, I‚Äôm using a local instance of LLaVA, an open-source model that can understand both text and images:\n\nimport dspy\nfrom typing import Literal\n\n# Configure our local LLaVA model\nlm = dspy.LM(\"ollama/llava:7b\", api_base=\"http://127.0.0.1:11434/\", api_key=\"\")\ndspy.configure(lm=lm)\n\nNext, we define all possible service categories. This ensures our system can only assign requests to real departments:\n\nServiceNameLiteral = Literal[\n¬†¬†¬† \"Outdoor Dining\", \"Streetlights\", \"Litter\",\n¬†¬†¬† \"Broken Park Equipment\", \"Damaged Sign\",\n¬†¬†¬† # ... and so on for all services\n]\n\nThe heart of the system is its understanding of how to process requests. We provide clear instructions through a system prompt:\n\nSYSTEM_PROMPT = \"\"\"You are a municipal services assistant. Analyze the request\ndescription and image to create a detailed description for city workers.\nGUIDELINES:\n- Use professional, factual language\n- Include details from both text and images\n- Remove subjective or emotional language\n- Focus on observable facts and specific details\n- Include measurements or quantities when visible\n- Note any accessibility or safety impacts\n\"\"\"\n\nNow for the clever part. We create a Signature that tells DSPy exactly what we expect as input and output:\n\nclass EnhancerSignature(dspy.Signature):\n¬†¬†¬† \"\"\"Improve the description based on the original description and image.\"\"\"\n¬†¬†¬† image = dspy.InputField(desc=\"The image showing the issue\")\n¬†¬†¬† original_description = dspy.InputField(desc=\"The original description text\")\n¬†¬†¬† enhanced_description = dspy.OutputField(desc=\"Enhanced actionable description\")\n¬†¬†¬† recommended_service_category = dspy.OutputField(desc=\"Recommended service category\")\n\nFinally, we can process actual 311 requests. Here‚Äôs how it works in practice:\n\n# Initialize our predictor\npredictor = dspy.Predict(EnhancerSignature)\n# Process a new request\nimage = dspy.Image.from_url(\"path_to_image.jpg\")\ndescription = \"Garbage on sidewalk\"\nresult = predictor(\n¬†¬†¬† image=image,\n¬†¬†¬† original_description=description\n)\n\nWhen a resident submits a request about garbage on the sidewalk and includes a photo, our system:\n\nAnalyzes both the text description and the image\nCreates a detailed, professional description for city workers\nAutomatically selects the correct service category\nReturns all this information in a structured format\n\nThe full implementation, along with example code and detailed documentation, is available in the Smart 311 System repository. The repository includes both a simple example for understanding the core concepts and a more complete implementation showing how to structure a production-ready system.\nWhat‚Äôs great about DSPy is its flexibility and consistency. It‚Äôs able to handle complex prompt engineering and chain-of-thought reasoning behind the scenes, letting us build a reliable system for 311 users.\nA brief example\nTo demonstrate this concept, we can use a real-world example. Here‚Äôs a request from my city‚Äôs 311 https://311.boston.gov/tickets/101005820254:\n\nThis is what the unformatted request looks like (i.e.¬†what is received before it turns into a nice webpage):\n[\n  {\n    \"service_request_id\": \"101005820254\",\n    \"service_name\": \"Requests for Street Cleaning\",\n    \"service_code\": \"Public Works Department:Street Cleaning:Requests for Street Cleaning\",\n    \"description\": \"Garbage on sidewalk\",\n    \"address\": \"19 Worcester St, Roxbury, Ma, 02118\",\n    \"media_url\": \"https://spot-boston-res.cloudinary.com/image/upload/v1734138056/boston/production/x1vhcizd7nvzb7xdzkfe.jpg#spot=e8762677-4564-41c3-b58b-3ddd206801cb\"\n  }\n]\nThe model workflow takes that request, considers the image and text, and returns the following:\n{\n  \"original_service\": \"Requests for Street Cleaning\",\n  \"recommended_service\": \"Requests for Street Cleaning\",\n  \"updated_description\": \"The area is littered with trash and debris. A blue box is present on the sidewalk by 19 Worcester St, Roxbury, Ma, 02118, which needs to be removed\",\n  \"emergency\": false,\n  \"image_verified\": true,\n  \"confidence\": 0.85,\n  \"rationale\": \"The image shows a sidewalk with litter scattered around, which is consistent with the current service classification of \\\"Requests for Street Cleaning\\\". The presence of trash on the sidewalk indicates that it requires attention from the city's street cleaning services to maintain cleanliness and hygiene in public spaces.\",\n  \"service_request_id\": \"101005820254\",\n  \"address\": \"19 Worcester St, Roxbury, Ma, 02118\",\n}\nIt verified the correct classification, updated the description, and correctly identified that it was not an emergent issue. It also provided a more detailed rationale and rated how confident it was with the updated classification and description.\nHere‚Äôs what to consider\nWith any AI-powered technology, cities need to approach them carefully. Public trust is essential and any system that uses AI needs to be accurate and transparent. Here, cities would audit it for bias and performance reviews should be part of any implementation plan.\nAlso, while compute costs continue to drop, cities should weigh the benefits of implementing something like this against waiting for vendors to integrate these features into standard 311 offerings.\nThe goal, of course, isn‚Äôt to replace human judgment but to augment it, allowing 311 staff to focus on what matters most, responding the requests from residents.\nBy weighing the tradeoffs of AI thoughtfully, cities can build systems that are more efficient and responsive to community needs."
  },
  {
    "objectID": "posts/building-ai-power-app-for-government/index.html",
    "href": "posts/building-ai-power-app-for-government/index.html",
    "title": "What I Learned Building CityUpdate: An AI-Powered Data Product for Public Sector Transparency",
    "section": "",
    "text": "CityUpdate\n\n\ntl;dr I built CityUpdate Boston with the goal to make Boston‚Äôs municipal performance metrics accessible and understandable to every resident using Large Language Models (LLMs). I learned a lot about where the real barriers to developing with AI are in government.\nThe Public Sector AI Challenge\nBuilding AI products for government presents unique challenges. Unlike private sector business apps (think Microsoft 365‚Äôs Copilot), public sector deployments face more scrutiny and have to clear a higher bar for reliability and transparency. Which makes sense- when we‚Äôre dealing with public services and taxpayer resources, we need to be extra careful about how we implement AI solutions.\nChoosing to Build\nA key decision in any tech project is whether to build or buy a solution (refer to 18F‚Äôs De-risking Guide for more information). For CityUpdate, building the system proved to be the better choice for several reasons:\n\nRisk Management: Building in-house gave me complete control over deployment.\nTesting & Evaluation: I could implement thorough testing, evaluation, and verification processes.\nDeployment Control: Full authority over go/no-go decision.\nCost Efficiency: The entire system costs only about $0.02 per day to run queries. Compare this to the 10s of thousands contractors charge government to build apps.\n\nThe Foundation: Quality Data Engineering\nOne major advantage I had was Boston‚Äôs existing robust data engineering pipelines. This highlights a crucial point about GenAI projects: they‚Äôre only as good as the data they‚Äôre built on. The city‚Äôs established infrastructure provided reliable, high-quality data - an essential foundation for any AI system.\nDevelopment was surprisingly swift - the entire build took just two days. This efficiency was possible because I was building on top of solid data infrastructure rather than starting from scratch.\nTransparency and Evaluation\nFrom the start, I wanted to bake in being transparent about:\n\nHow the system generates its summaries\nThe evaluation methods I used\nWhat makes it to publication and what doesn‚Äôt\n\nI implemented different types of evaluations to ensure the output meets defined standards for public consumption. This rigorous approach helps maintain trust and reliability in the system.\nKey Takeaways\n\nData Quality is Non-Negotiable: Good AI systems require good data and solid data engineering\nCost Management: Small-scale AI projects can be surprisingly affordable when well-designed\nTransparency First: Being open about AI generation and evaluation builds trust\nBuild vs Buy: Sometimes building in-house provides better control and risk management\n\nThe success of CityUpdate demonstrates that with careful planning, solid data foundations, and a commitment to transparency, cities can build AI-powered tools that genuinely serve the public good. It‚Äôs a small but significant step toward making government performance metrics more accessible to everyone.\nThis project represents an early exploration of how AI can enhance government transparency and public understanding. While the investment was modest, the potential impact on civic engagement and government accountability is substantial."
  },
  {
    "objectID": "posts/leaving-public-service/index.html",
    "href": "posts/leaving-public-service/index.html",
    "title": "I Didn‚Äôt Leave Public Service, Public Service Left Me: Why Inefficient Government Hiring Processes Drove Me Away",
    "section": "",
    "text": "When I graduated college with technical skills in statistics and computational social science, I was drawn to public service. A semester in the nation‚Äôs capital and deep dive into Street-level Bureaucracy had convinced me that state and local government was where I could make the most meaningful impact. What I didn‚Äôt expect was how the very systems designed to bring talent into government would become the biggest obstacle to that goal.\n\nThe Modern Reality of Government Hiring\nJennifer Pahlka, author of ‚ÄúRecoding America,‚Äù recently shared insights about government hiring challenges on Bloomberg‚Äôs Odd Lots podcast. Her observations about federal hiring resonated deeply with my experience, though I‚Äôve found these issues run even deeper at state and local levels.\nThe problems she identified are systemic:\n1. Software systems that the private sector abandoned long ago\n2. Evaluation processes that fail to assess actual capabilities\n3. The exclusion of hiring managers from candidate screening\n4. Outdated job descriptions that don‚Äôt reflect current roles\n5. HR processes that reward candidates who simply copy-paste job descriptions rather than those who honestly assess their skills\nThese challenges are amplified in state and local government. While federal agencies are implementing reforms through the Chance to Compete Act and SME-QA process pilots, state and local governments lack standardized guidance. With thousands of agencies developing independent hiring approaches, the result is a fragmented system with minimal oversight and few opportunities for innovation.\n\n\nMy Journey Through the Bureaucratic Maze\nMy experience with DC government‚Äôs hiring system illustrated these challenges perfectly. Their HRMS (PeopleSoft) suffered from persistent technical issues ‚Äì broken links to position descriptions forced hiring managers to create workarounds just to advertise openings. The application process included the thing that bothers everyone when applying for positions: candidates would upload their resumes, only to manually input the same information again. Their process also required candidates to self-assess their skills, disincentivizing honesty for those who know who the system works.\nLater, when I transitioned to the hiring side, I witnessed how these inefficiencies created new problems. Some agencies began relying on contractors instead of full-time employees to bypass HR departments. While this offered a temporary solution, it introduced new risks and potential biases into the hiring process.\n\n\nThe Same Experiences Across Governments\nMy most recent attempt to return to public service in Massachusetts further highlighted these systemic issues. After relocating with my family, I encountered a series of discouraging experiences: multiple applications disappeared into weeks of silence.\nIn one case, I advanced through several interview rounds and was selected as the top candidate, only to have the opportunity evaporate due to a sudden state-wide hiring freeze. In another instance, despite an internal recommendation, I waited months for a position to open, completed one interview expecting another to follow, and then received a rejection after four weeks without any feedback.\n\n\nThe Private Sector Contrast\nThe contrast became clear when I explored private sector opportunities. A consulting firm completed their entire hiring process ‚Äì including skills assessments and consistent communication ‚Äì in three weeks. This efficiency wasn‚Äôt merely about speed; it reflected a modern understanding of how to use skills-based evaluations and secure talented candidates.\n\n\nLooking Forward\nWhile organizations like Work for America are beginning to address these challenges, their impact hasn‚Äôt yet reached the thousands of state and local agencies that need it most. As someone with expertise in AI, machine learning, data governance, engineering, and evaluation, I found it increasingly difficult to contribute these valuable skills to public service through existing channels.\nThe irony is stark: systems intended to ensure fair and equitable hiring have become so rigid and inefficient that they‚Äôre driving away the talent they should attract. For government to effectively serve its citizens in an increasingly technical world, it needs hiring processes that can keep pace with evolving technology and the changing nature of public service work.\nIt‚Äôs crucial to note that this critique of hiring systems comes at a particularly challenging time for public servants across all levels of government. Many dedicated civil servants are facing unprecedented pressures and uncertainty about their roles. My criticism is directed solely at outdated systems and processes that make it difficult to bring new talent into government ‚Äì not at the countless committed individuals who work tirelessly to serve the public good despite these obstacles.\nThis isn‚Äôt about giving up on public service ‚Äì it‚Äôs about recognizing when systems have become barriers rather than gateways to public impact. Until these fundamental issues are addressed, talented individuals with technical skills and a genuine desire to serve the public good will continue finding themselves redirected to the private sector, not by choice, but by necessity."
  }
]