[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Hersh and I‚Äôm a data scientist based in Boston, MA and I love to use data to make an impact on policy, processes, and government.\nAll opinions expressed in this blog represent my own.\nReach out to me here! üëáüèæ:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Local Vision-Language Model Experimentation for Public Sector Use Cases\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\nAI\n\n\n\n\n\n\n\n\n\nNov 27, 2024\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Most State and Local Generative AI Efforts Won‚Äôt Be as Impactful as Promised\n\n\n\n\n\n\nGovernment\n\n\nAI\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nObservable‚Äôs New Framework: A Better Way for Cities to Tell Data Stories\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nHow Cities Can Create Better Data Interfaces: Lessons Learned from Redesigning RentSmart Boston\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nHow government analytics teams can embrace #PublicMoneyPublicCode\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nApr 2, 2023\n\n\nHersh Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nCity Government Open Source Efforts, Ranked by Their Github Stats\n\n\n\n\n\n\nOpen Source\n\n\nGovernment\n\n\n\n\n\n\n\n\n\nDec 22, 2021\n\n\nHersh Gupta\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/city-government-open-source/index.html",
    "href": "posts/city-government-open-source/index.html",
    "title": "City Government Open Source Efforts, Ranked by Their Github Stats",
    "section": "",
    "text": "In 2016, then US Chief Information Officer Tony Scott announced the launch code.gov, stating\n\n‚ÄúBy harnessing 21st Century technology and innovation, we‚Äôre improving the Federal Government‚Äôs ability to provide better citizen-centered services and are making the Federal Government smarter, savvier, and more effective for the American people‚Ä¶We also envision it becoming a useful resource for State and local governments and developers looking to tap into the Government‚Äôs code to build similar services, foster new connections with their users, and help us continue to realize the President‚Äôs vision for a 21st Century digital government.‚Äù\n\nSince then, many cities have made the push to open source, with more and more opting to publish their code in public-facing repositories.\n\nOpen-Source Cities, Ranked\nI rank US municipal governments‚Äô in how well they have adapted to the new landscape of open source government code, with the following caveats:\n\nLimited to the 100 largest U.S. cities by population\nOfficial government Github profiles, no private third-party or volunteer organizations\nPublic repositories only\n\n\n\n\n\n\n\n\n\nHow this works: City open-source rankings are calculated by analyzing public metrics available via the Github Repository Metrics API endpoint. Scores are calculated by combining all repository metrics, including total number of forks, watchers, and commits along with the total number of public repositories on each city‚Äôs Github profile. Code for this analysis is available on Github here."
  },
  {
    "objectID": "posts/government-analytics-oss/index.html",
    "href": "posts/government-analytics-oss/index.html",
    "title": "How government analytics teams can embrace #PublicMoneyPublicCode",
    "section": "",
    "text": "The push for Public Money, Public Code started in Europe with the aim that government software developed for the public sector should be made publicly available via a Free and Open Source Software (FOSS) license. In fact, there are economic, moral, transparency, and participatory related reasons why governments should use and publish open source software. Recently, bipartisan legislation has been introduced in the US Senate that would, among other things, ‚Äúcodify open source software as public infrastructure.‚Äù\nMuch of the focus of Public Money, Public Code has been to allow government agencies to develop and contribute to open source software applications. Government analytics teams, on the other hand, do not consider themselves software developers. However, the same way that scientific research funded via public grants is made publicly available, government analytics teams can opt to use, promote, and orchestrate with open source tools. Thus, they can reap the same benefits as other organizations that have chosen to be a part of the open source community.\nA ‚Äúdata stack‚Äù is a set of interoperable tools used for analytic data work. The table below shows open source alternatives to proprietary analytics tools:\n\nOSS Alternatives\n\n\n\n\n\n\n\nPurpose\nProprietary Tools\nOpen Source Tools\n\n\n\n\nInfrastructure Orchestration\nAnsible\nDocker, Kubernetes\n\n\nResearch\nSTATA, SAS, SPSS\nR, Python\n\n\nBusiness Intelligence (Internal)\nTableau, PowerBI, Microstrategy\nApache Superset\n\n\nDashboards (External)\nTableau, ArcMap, PowerBI\nJavaScript\n\n\nGIS\nArcGIS Desktop\nPostGIS, Apache Sedona\n\n\nETL/Pipelines\nInformatica, Tableau Prep\nAirflow, Dagster, Meltano\n\n\nAnalytical Query Engine\nPL/SQL\nTrino\n\n\nReal-Time Analytics\n?\nApache Pinot\n\n\n\nUsing an open source data stack can help government analytics teams embrace #PublicMoneyPublicCode. Many proprietary/paid/‚ÄúEnterprise‚Äù tools will claim to be more stable and secure than open source tools. However, open source tools are not inherently insecure, as open source technology is becoming the bedrock of more and more Enterprise tools.\nThe cash cow of companies that develop closed-source software is the generalization that ‚Äúgovernment users are more risk-averse and will stick with closed source software.‚Äù This is best refuted by the fact that the US Department of Defense has published a Frequently Asked Questions page on open source software and states ‚ÄúSome OSS is very secure, while others are not; some proprietary software is very secure, while others are not. Each product must be examined on its own merits.‚Äù As the US Department of Defense has publicly embraced OSS, so too can government analytics teams at all levels of government."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "3 Tips for Better Data Analysis Using Strategies from Software Development\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#borrow-from-agile",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#borrow-from-agile",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "1. Borrow from Agile",
    "text": "1. Borrow from Agile\nAgile is a process framework that focuses on iterative development done in short bursts, called sprints\nPlanning and prioritization at the start of each sprint\nUse regular scrum meetings to conduct sprint planning, prioritization, and review\nClearly defining tasks with deliverables and timelines\nPresent hypotheses, approach(es), and aspects of data product(s)\nRetrospectives and demos at the end of each sprint\nPresent analyses/data products to stakeholders, review sprint"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#use-readmes",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#use-readmes",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "2. Use READMEs",
    "text": "2. Use READMEs\nA README is documentation that tells you how to start using and understanding a new piece of software\nA typical README is a text or markdown-formatted file included with the project\nLearn how to format a README here: https://www.makeareadme.com/\nREADMEs for data products include things like usage, reproducibility, sources, and metadata"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#integrate-user-experience-testing",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#integrate-user-experience-testing",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "3. Integrate User Experience Testing",
    "text": "3. Integrate User Experience Testing\nFor a widely-used data product, consider User Stories\nUser Stories are a prioritized list of tasks the user is trying to accomplish\nUser Stories help developers, analysts understand users‚Äô needs\nIterative, human centered design\nIs the analysis interpretable? Is the way it‚Äôs presented intuitive and simple?\nPrototype analysis with user groups, have them ask questions, document findings"
  },
  {
    "objectID": "talks/software-development-tips-for-better-data-analysis/index.html#tips-for-better-data-analysis",
    "href": "talks/software-development-tips-for-better-data-analysis/index.html#tips-for-better-data-analysis",
    "title": "3 Tips for Better Data Analysis Using Strategies from Software Development",
    "section": "Tips for Better Data Analysis",
    "text": "Tips for Better Data Analysis\n\nBorrow from Agile\n\nUse READMEs\n\nIntegrate User Experience Testing"
  },
  {
    "objectID": "posts/redesigning-rent-smart-boston/index.html",
    "href": "posts/redesigning-rent-smart-boston/index.html",
    "title": "How Cities Can Create Better Data Interfaces: Lessons Learned from Redesigning RentSmart Boston",
    "section": "",
    "text": "tl;dr: redesigning the Boston RentSmart dashboard helped me understand when and why cities should choose to build custom web interfaces for datasets.\nThe early 2010s featured a surge of ‚Äúopen data‚Äù ‚Äì from government datasets to public research. But while this wealth of data has potential value, a question remains around how to make it truly useful and accessible.\nOn one hand, data analysts rely on tools to efficiently process datasets, but these rarely enable building new interfaces. They use basic publishing tools to share findings. Web developers, on the other hand, focus on crafting engaging interfaces but often lack access to rich datasets. There‚Äôs a divide between these roles - one cares about data maintenance, the other about display. Bridging this gap between data and design experts is key to unlocking the promise of open data for the public good.\nTo personally bridge this gap in my own skillset, I learned enough web development to redesign the City of Boston‚Äôs RentSmart Dashboard. For some context, The City of Boston provides compiled data on housing violations, building violations, enforcement violations, housing complaints, sanitation requests, and/or civic maintenance requests. The City releases this data through¬†Analyze Boston¬†and they provide a¬†Tableau dashboard¬†to allow users to search the data.\nI redesigned this dashboard in a modern web framework, with similar functionality to the original one. Here‚Äôs what I learned:\n\nInvesting time in custom UI and UX is worthwhile if you care about user experience. Building an interface from scratch takes more effort upfront, but offers far more flexibility to craft an intuitive, user-friendly product. Off-the-shelf BI tools often lack the polish and refinements of a bespoke web app.\nWeb development scales better long-term than relying solely on point-and-click BI tools. Web apps can be optimized for performance, accessibility, SEO, and more through standard practices. BI tools don‚Äôt often receive this level of optimization. Additionally, thanks to reusable code patterns, each new web app moves faster than starting a dashboard from scratch. The reproducibility of code compounds over time.\nWeb frameworks promote collaboration and community growth. By using open source tools and publishing my code, I can get feedback, bug reports, and contributions from other developers. The open source community helps improve the quality and security of the product through peer review and knowledge sharing. Proprietary BI tools tend to have less community support.\n\nBy diving into web development to create a more engaging open data resource, I gained hands-on experience bridging the gap between my data analysis skills and ability to build great interfaces. Custom web development ultimately creates a better user experience and offers more scalability than inflexible, out-of-the-box BI tools alone."
  },
  {
    "objectID": "posts/city-data-stories-with-observable/index.html",
    "href": "posts/city-data-stories-with-observable/index.html",
    "title": "Observable‚Äôs New Framework: A Better Way for Cities to Tell Data Stories",
    "section": "",
    "text": "Testing Observable 2.0 with Boston's 311 Data\n\n\nCities today want to leverage their vast amounts of open data to tell compelling stories that inform, optimize operations, and engage citizens. However, it is too expensive for most cities to invest in entire web teams to create customized dashboards. The answer lies in open source software and embracing code. Open source options allow cities to save significantly on software licensing costs, while a code based approach facilitates reproducibility and, thus, economies of scale.\nOne open source framework that is gaining popularity for data applications is Observable. Observable combines the power of languages like Python, R, and SQL that data scientists rely on with JavaScript, the native language of the web. This allows cities to create highly dynamic and interactive data visualizations that live natively on the web, removing the need for proprietary business intelligence or visualization software.\nWith Observable 2.0, interfaces are built locally and available to deploy cites‚Äô own servers. This way, users don‚Äôt have to transmit sensitive data to third-party servers. Instead, they maintain full control over security and privacy while still producing powerful analytics.\n\n\n\nWhich URL is SEO optimized? Which would you expect to load more quickly?\n\n\nThe new Observable Framework is also extremely customizable, enabling cities to build precisely the dashboards and data stories they wish to tell, optimized specifically for their use cases. Compared to traditional government software options like ArcGIS and Tableau, Observable provides better performance at a fraction of the cost. With reusable data analysis and visualization components, cities can quickly build multiple data applications on top of the same foundational codebase. This modular approach paired with the latest web development techniques makes Observable a cost-effective way for cities to tell a story with their data.\nBy embracing open source software like Observable, cities access ready-made tools for success while benefiting from standardized best practices. Observable allows cities to tell rich, interactive analytic narratives with their public sector data in a secure, performant manner that simply isn‚Äôt possible with other approaches. Any city looking to innovate with data visualization and storytelling would do well to explore this new framework."
  },
  {
    "objectID": "posts/state-local-gov-unready-for-genAI/index.html",
    "href": "posts/state-local-gov-unready-for-genAI/index.html",
    "title": "Why Most State and Local Generative AI Efforts Won‚Äôt Be as Impactful as Promised",
    "section": "",
    "text": "I'm a data scientist working on AI in city government. I think hype around generative AI in the public sector is causing more issues than it promises to solve.\nDevelopments in generative artificial intelligence seem promising in improving government operations, like streamlining routine bureaucratic tasks or summarizing documents for information-seekers. However, the reality is that most state and local governments lack the expertise, safeguards, and tech leadership to implement products or processes that integrate generative AI.\nDespite the unpreparedness, cities are jumping on the generative AI bandwagon. It's spurred by technology managers falling for flashy claims of cutting-edge public sector innovations from B2G vendors.\nA common trope in government is that government technology leaders don't understand the \"business\" of government, i.e., they are too far removed from the day-to-day of government services. This leads to AI chatbots being seen by these leaders as a sort of panacea to most government challenges, without understanding the nuances of a given problem.\nIt's complicated further by the fact that few, if any, government technology managers deeply understand the underlying tech of generative AI. The implication here is that, while the typical gov tech leader is aware that genAI has drawbacks, leaders lack the knowledge to adequately measure or mitigate them. New York City's recent genAI scandal serves as a prime example.\nBecause of this, the public should be more skeptical of the people in government who push generative AI technologies without understanding them or the problems they're actually able to solve.\nRather than chasing the latest AI innovations, govtech managers would be better served prioritizing foundational data management practices - establishing centralized governance, systematizing data collection/operations, and thoughtfully designing core IT infrastructure.\nHowever, hype around AI and other advanced technologies creates pressure on local leaders to implement them before doing the foundational work. The allure of AI can push limited resources toward shiny new projects rather than the admittedly boring work of improving core systems and infrastructure.\nIf asked whether they should hire another database administrator or an procure some AI software, too many gov leaders would be mistakenly convinced the smarter investment would be expanding AI capacity.\nIn their rush to keep up with innovation for innovation's sake, many states and cities will try to leapfrog over essential steps and end up with serious consequences for the public and government operations. Potentially useful but unreliable AI systems, when deployed without appropriate forethought, will only amplify existing operational inefficiencies rather than resolving them.\nReal innovation demands some level of care. Resisting the temptation to compulsively chase every shiny new AI object will yield more substantial, long-term dividends for residents."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html",
    "href": "posts/local-lmms-in-government/index.html",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "",
    "text": "Computer vision has evolved many times, from basic handwriting pattern recognition to object detection. Now, with the emergence of Large Multimodal Model (LMM), it‚Äôs experiencing another dramatic transformation. LMMs can now process images and generate descriptions, insights, analysis and even new prompts for other models. Large (and even mid-sized) tech companies have the resources to deploy these models in the cloud and support them with teams dedicated to their upkeep. Government, however, has long lacked this luxury ‚Äì there are very few ML/AI teams in government, and where they exist are in already well-funded agencies and organizations. Therefore, there‚Äôs a compelling case for experimenting with these models while taking the lowest-cost, lowest-risk approach."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#vision-models-in-government",
    "href": "posts/local-lmms-in-government/index.html#vision-models-in-government",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Vision Models in Government",
    "text": "Vision Models in Government\nGovernments sit on goldmines of visual data: from infrastructure inspection photos to document processing queues, from aerial imagery to camera feeds. These are another form of administrative data, the utility of which often gets overlooked. The potential for automation and efficiency gains is plenty. For example, a vision model could rapidly assess road damage from maintenance photos, extract data from handwritten forms, or analyze aerial imagery for urban planning.\nWhile the public sector would benefit from any of these applications, it often lacks the technical resources to implement them effectively. Government agencies typically employ few experts or engineers compared to the private sector, and the collaboration between technical teams and services designers is not as tightly coupled in the public sector as it is in the private sector."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#the-case-for-local-experimentation",
    "href": "posts/local-lmms-in-government/index.html#the-case-for-local-experimentation",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "The Case for Local Experimentation",
    "text": "The Case for Local Experimentation\nCloud platform LMM offerings, while powerful, come with significant costs that can strain public sector budgets. Each Rekognition or AI Vision API call adds up, making experimentation and proof-of-concept development prohibitively expensive. This is compounded by the fact that government agencies do not have large, experienced engineering teams, so they end up spend more time spinning their wheels while accruing these costs.\nThis is where local deployment of models becomes a practical alternative. Local vision models, specifically, offer several advantages:\n\nCost-effective experimentation without charges for each inference/query\nComplete control over data privacy and security\nFreedom to modify and fine-tune models for specific use cases\n\nThere are a few downsides: you can‚Äôt use proprietary models (GPT/Claude are off the table), you need to have the right hardware (but honestly, if you‚Äôre already buying a $2K computer for a new hire, might as well throw in a GPU), and it‚Äôs not as easy as point-and-click interfaces."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#lmm-public-sector-applications",
    "href": "posts/local-lmms-in-government/index.html#lmm-public-sector-applications",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "LMM Public Sector Applications",
    "text": "LMM Public Sector Applications\nThe applications for vision models in government services are varied:\n\nGenerating Service Tickets: Analyzing images of incidents or areas needing service and preparing reports to 311\nInfrastructure Maintenance: Analyzing photos of parks, roads, and buildings to identify maintenance needs\nEnvironmental Monitoring: Processing satellite and drone imagery to track environmental changes\nAccessibility Services: Providing image descriptions for visually impaired citizens accessing government websites"
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#example-open-source-vision-intelligence-with-llava",
    "href": "posts/local-lmms-in-government/index.html#example-open-source-vision-intelligence-with-llava",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Example: Open Source Vision Intelligence with LLaVA",
    "text": "Example: Open Source Vision Intelligence with LLaVA\nThere are hundreds of open-source options for image-to-text models. However, LLaVA is one that stands out for its capabilities and accessibility. Built on Meta‚Äôs LLaMA language model, LLaVA uses specialized components to translate images into a format the underlying language model can process, allowing it to have natural conversations about visual content.\nLLaVA-NeXT represents a significant step forward in making local vision models more accessible. It uses the latest family of LLaMA models, ‚Äúwith improved reasoning, OCR, and world knowledge.‚Äù"
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#proof-of-concept-experiments",
    "href": "posts/local-lmms-in-government/index.html#proof-of-concept-experiments",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Proof-of-Concept Experiments",
    "text": "Proof-of-Concept Experiments\nI tested LLaVA-NeXT on two common municipal workflows: 311 service requests and playground safety inspections. Both experiments used specialized system prompts to structure the model‚Äôs analysis.\n\n311 Service Request Processing\nThe model converted photos into structured tickets by extracting location data, classifying issues, and generating detailed descriptions.\n\n\n\nTesting LLaMA-NeXT with picture of traffic in lmstudio.ai\n\n\nIn testing with traffic obstruction photos, it correctly identified violations and provided comprehensive situation assessments matching human operator quality.\n\n\nPlayground Safety Inspections\nFor park maintenance, the model performed systematic equipment evaluations. When analyzing playground components, it reliably assessed material condition, structural integrity, and potential safety hazards.\n\n\n\nTesting LLaMA-NeXT with picture of a playground in lmstudio.ai\n\n\nThe output followed given inspection protocols, providing maintenance teams with actionable documentation.\n\n\nKey Findings\nLocal deployment of LLaVA-NeXT proved effective, with several advantages:\n\nFast processing time on consumer hardware (my laptop‚Äôs GPU)\nConsistent output formatting suitable for existing workflows\nSuccessful domain adaptation through prompt engineering\nNo additional training required for municipal use cases\n\nThese experiments demonstrate that local LMMs can effectively augment government workflows while maintaining control over data and infrastructure."
  },
  {
    "objectID": "posts/local-lmms-in-government/index.html#conclusion",
    "href": "posts/local-lmms-in-government/index.html#conclusion",
    "title": "Local Vision-Language Model Experimentation for Public Sector Use Cases",
    "section": "Conclusion",
    "text": "Conclusion\nThe democratization of vision model technology through local deployment options opens exciting possibilities for public sector innovation. While cloud-based solutions will continue to play a crucial role, the ability to experiment locally removes significant barriers to entry for government agencies looking to improve their services.\nBy starting with focused proof-of-concept projects using tools like LLaVA-NeXT, public sector organizations can build the expertise and confidence needed to implement these transformative technologies at scale. The key is to begin with manageable experiments that demonstrate clear value while building internal capacity for larger implementations.\nCover image (c) karyative, SmashIcons"
  }
]